{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SGX YFinance Data Extraction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMMgTLv0rWXx"
      },
      "source": [
        "from datetime import datetime\n",
        "import lxml\n",
        "from lxml import html\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import sys\n",
        "import traceback\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from random import choice\n",
        "import xlsxwriter\n",
        "\n",
        "# Libraries required to limit the time taken by a request\n",
        "import signal\n",
        "from contextlib import contextmanager\n",
        "\n",
        "class TimeoutException(Exception): pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwzRTdNorigq"
      },
      "source": [
        "def get_page(url):\n",
        "    # Set up the request headers that we're going to use, to simulate\n",
        "    # a request by the Chrome browser. Simulating a request from a browser\n",
        "    # is generally good practice when building a scraper\n",
        "    headers = {\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',\n",
        "        'Accept-Encoding': 'gzip, deflate, br',\n",
        "        'Accept-Language': 'en-US,en;q=0.9',\n",
        "        'Cache-Control': 'max-age=0',\n",
        "        'Pragma': 'no-cache',\n",
        "        'Referrer': 'https://google.com',\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'\n",
        "    }\n",
        "    return requests.get(url, headers=headers)\n",
        "\n",
        "def parse_rows(table_rows):\n",
        "    parsed_rows = []\n",
        "\n",
        "    for table_row in table_rows:\n",
        "        parsed_row = []\n",
        "        el = table_row.xpath(\"./div\")\n",
        "\n",
        "        none_count = 0\n",
        "\n",
        "        for rs in el:\n",
        "            try:\n",
        "                (text,) = rs.xpath('.//span/text()[1]')\n",
        "                parsed_row.append(text)\n",
        "            except ValueError:\n",
        "                parsed_row.append(np.NaN)\n",
        "                none_count += 1\n",
        "\n",
        "        if (none_count < 4):\n",
        "            parsed_rows.append(parsed_row)\n",
        "            \n",
        "    return pd.DataFrame(parsed_rows)\n",
        "\n",
        "def clean_data(df):\n",
        "    df = df.set_index(0) # Set the index to the first column: 'Period Ending'.\n",
        "    df = df.transpose() # Transpose the DataFrame, so that our header contains the account names\n",
        "    \n",
        "    # Rename the \"Breakdown\" column to \"Date\"\n",
        "    cols = list(df.columns)\n",
        "    cols[0] = 'Date'\n",
        "    df = df.set_axis(cols, axis='columns', inplace=False)\n",
        "    df[\"Date\"] = df[\"Date\"].astype(str)\n",
        "    \n",
        "    numeric_columns = list(df.columns)[1::] # Take all columns, except the first (which is the 'Date' column)\n",
        "\n",
        "    for column_index in range(1, len(df.columns)): # Take all columns, except the first (which is the 'Date' column)\n",
        "        df.iloc[:,column_index] = df.iloc[:,column_index].str.replace(',', '') # Remove the thousands separator\n",
        "        df.iloc[:,column_index] = df.iloc[:,column_index].astype(np.float64) # Convert the column to float64\n",
        "        \n",
        "    return df\n",
        "\n",
        "def scrape_table(url):\n",
        "    # Fetch the page that we're going to parse\n",
        "    page = get_page(url);\n",
        "\n",
        "    # Parse the page with LXML, so that we can start doing some XPATH queries\n",
        "    # to extract the data that we want\n",
        "    tree = html.fromstring(page.content)\n",
        "\n",
        "    # Fetch all div elements which have class 'D(tbr)'\n",
        "    #table_rows = tree.xpath(\"//div[contains(@class, 'D(tbr)')]\")\n",
        "    \n",
        "    table_rows = tree.xpath(\"//div[contains(@class, 'fin-row')]\")\n",
        "\n",
        "    # Ensure that some table rows are found; if none are found, then it's possible\n",
        "    # that Yahoo Finance has changed their page layout, or have detected\n",
        "    # that you're scraping the page.\n",
        "\n",
        "    try:\n",
        "      assert len(table_rows) > 0\n",
        "\n",
        "      df = parse_rows(table_rows)\n",
        "      df = clean_data(df)\n",
        "    except AttributeError:\n",
        "      print(\"Code not found : \"+code)\n",
        "      return\n",
        "    except IndexError:\n",
        "      print(\"Financials not found : \"+code)\n",
        "      return\n",
        "    except AssertionError: \n",
        "      # https://stackoverflow.com/questions/11587223/how-to-handle-assertionerror-in-python-and-find-out-which-line-or-statement-it-o\n",
        "      _, _, tb = sys.exc_info()\n",
        "      traceback.print_tb(tb) # Fixed format\n",
        "      tb_info = traceback.extract_tb(tb)\n",
        "      filename, line, func, text = tb_info[-1]\n",
        "\n",
        "      print('An error occurred on line {} in statement {}'.format(line, text))\n",
        "      return\n",
        "\n",
        "    return df\n",
        "\n",
        "def scrape(symbol):\n",
        "    print('Attempting to scrape data for ' + symbol)\n",
        "\n",
        "    df_balance_sheet = scrape_table(\n",
        "        'https://finance.yahoo.com/quote/' + symbol + '/balance-sheet?p=' + symbol\n",
        "        )\n",
        "    df_income_statement = scrape_table(\n",
        "        'https://finance.yahoo.com/quote/' + symbol + '/financials?p=' + symbol\n",
        "        )\n",
        "    df_cash_flow = scrape_table(\n",
        "        'https://finance.yahoo.com/quote/' + symbol + '/cash-flow?p=' + symbol\n",
        "        )\n",
        "\n",
        "    try: \n",
        "      df_balance_sheet = df_balance_sheet.set_index('Date')    \n",
        "      df_income_statement = df_income_statement.set_index('Date')    \n",
        "      df_cash_flow = df_cash_flow.set_index('Date')\n",
        "    except: \n",
        "      return None\n",
        "    \n",
        "    df_joined = df_balance_sheet \\\n",
        "        .join(df_income_statement, on='Date', how='outer', rsuffix=' - Income Statement') \\\n",
        "        .join(df_cash_flow, on='Date', how='outer', rsuffix=' - Cash Flow') \\\n",
        "        .dropna(axis=1, how='all') \\\n",
        "        .reset_index()\n",
        "            \n",
        "    df_joined.insert(1, 'Symbol', symbol)\n",
        "    \n",
        "    return df_joined\n",
        "\n",
        "def scrape_multi(symbols):\n",
        "    df_combined = pd.concat([scrape(symbol) for symbol in symbols], sort=False)\n",
        "    df_combined.dropna(subset = ['index'])\n",
        "    df_combined = df_combined.loc[df_combined[\"Date\"] != \"ttm\"]\n",
        "    df_combined.drop(columns=['index'])\n",
        "    df_combined['Date'] = df_combined['Date'].dt.date\n",
        "    # df_combined[\"Date\"] = pd.to_datetime(df_combined[\"Date\"], dayfirst = True)\n",
        "    df_combined.reset_index()\n",
        "\n",
        "    return df_combined\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EotLEwXroEv"
      },
      "source": [
        "df = pd.read_csv(\"SESprice.dat\", \n",
        "                 sep=\";\",  \n",
        "                 usecols=[2,11,14], \n",
        "                 names=['Status','Board','Symbols'])\n",
        "#trim the leading and trailing spaces of the columns\n",
        "data_frame_trimmed = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
        "#filter only the mainboard listed companies & not suspended & not delisted\n",
        "data_frame_trimmed = data_frame_trimmed.loc[(data_frame_trimmed[\"Board\"] == \"MAINBOARD\") & (data_frame_trimmed[\"Status\"] != \"SUSP\") & (data_frame_trimmed[\"Status\"] != \"DL\") ]\n",
        "data_frame_trimmed['Symbols'] = data_frame_trimmed['Symbols'].astype(str) + '.SI'\n",
        "symbols = data_frame_trimmed['Symbols'].tolist()\n",
        "df_combined = scrape_multi(symbols)\n",
        "df_combined.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ycw4Ud5srsm6"
      },
      "source": [
        "date = datetime.today().strftime('%Y-%m-%d')\n",
        "writer = pd.ExcelWriter('Yahoo-Finance-Scrape-' + date + '.xlsx')\n",
        "df_combined.to_excel(writer, index = False)\n",
        "writer.save()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}